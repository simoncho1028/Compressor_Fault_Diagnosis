{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5749807",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import timedelta\n",
    "from scipy.signal import stft, resample\n",
    "from typing import Dict, List, Tuple, Any\n",
    "\n",
    "#####################################\n",
    "# 1. 설정 클래스: 모든 파라미터를 중앙에서 관리\n",
    "#####################################\n",
    "\n",
    "class Config:\n",
    "    \"\"\"\n",
    "    프로젝트의 모든 설정을 담는 클래스\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # STFT 파라미터\n",
    "        self.fs = 80000\n",
    "        self.nperseg = 1024\n",
    "        self.hop_length = self.nperseg // 4\n",
    "        self.freq_limit = 1000\n",
    "        self.target_time_bins = 16\n",
    "\n",
    "        # 윈도우 파라미터\n",
    "        self.window_size_atoms = 5\n",
    "        self.step = 1\n",
    "        self.window_duration_seconds = self.window_size_atoms * 0.0102\n",
    "\n",
    "        # 데이터 분할 파라미터\n",
    "        self.block_size_seconds = 20\n",
    "        self.train_ratio = 0.6\n",
    "        self.val_ratio = 0.2\n",
    "\n",
    "        # 입출력 디렉토리\n",
    "        self.output_stft_dir = \"processed_data_stft\"\n",
    "        self.output_raw_dir = \"processed_data_raw\"\n",
    "        \n",
    "        # 입력 파일 경로\n",
    "        self.normal_files = {f\"normal_{i}\": f\"raw_data/normal/normal_{i}.csv\" for i in range(1, 21)}\n",
    "        self.abnormal_files = {\n",
    "            \"abnormal_1\": {f\"abnormal_1_{i}\": f\"raw_data/abnormal/abnormal_1_{i}.csv\" for i in range(1, 4)},\n",
    "            \"abnormal_2\": {f\"abnormal_2_{i}\": f\"raw_data/abnormal/abnormal_2_{i}.csv\" for i in range(1, 3)},\n",
    "            \"abnormal_3\": {f\"abnormal_3_{i}\": f\"raw_data/abnormal/abnormal_3_{i}.csv\" for i in range(1, 3)},\n",
    "            \"abnormal_4\": {f\"abnormal_4_{i}\": f\"raw_data/abnormal/abnormal_4_{i}.csv\" for i in range(3, 6)},\n",
    "        }\n",
    "        self.all_files = {\"normal\": self.normal_files, **self.abnormal_files}\n",
    "        \n",
    "        # 증강 설정\n",
    "        self.classes_to_augment = ['abnormal_1', 'abnormal_2']\n",
    "\n",
    "#####################################\n",
    "# 2. 기본 유틸리티 함수: 데이터 로드, 원자 추출, 윈도우 생성 등\n",
    "#####################################\n",
    "\n",
    "def load_data(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"CSV 파일을 읽어 'time' 컬럼을 datetime으로 변환 후 인덱스로 설정\"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['time'] = pd.to_datetime(df['time'], errors='coerce')\n",
    "    df.set_index(\"time\", inplace=True)\n",
    "    return df\n",
    "\n",
    "def group_atoms(data: pd.DataFrame) -> List[pd.DataFrame]:\n",
    "    \"\"\"원자(Valid Pattern) 추출\"\"\"\n",
    "    data = data.copy().sort_index()\n",
    "    data['time_interval'] = data.index.to_series().diff().dt.total_seconds().fillna(0)\n",
    "    data['is_interval'] = (data['time_interval'] >= 0.7) & (data['time_interval'] <= 0.75)\n",
    "    data['is_vibration'] = data['time_interval'] < 0.1\n",
    "    data['group'] = data['is_interval'].cumsum()\n",
    "    \n",
    "    atoms = []\n",
    "    for _, group in data.groupby('group'):\n",
    "        if group['is_interval'].any() and group['is_vibration'].any():\n",
    "            atom = group[group['is_vibration']]\n",
    "            if len(atom) > 1:\n",
    "                atoms.append(atom)\n",
    "    return atoms\n",
    "\n",
    "def create_sliding_windows(atoms: List[pd.DataFrame], window_size: int, step: int) -> List[pd.DataFrame]:\n",
    "    \"\"\"원자 리스트로부터 슬라이딩 윈도우 생성\"\"\"\n",
    "    return [pd.concat(atoms[i:i+window_size]) for i in range(0, len(atoms) - window_size + 1, step)]\n",
    "\n",
    "def augment_window(window: pd.DataFrame, augment_factor: int, methods=['noise', 'shift', 'scaling']) -> List[pd.DataFrame]:\n",
    "    \"\"\"주어진 윈도우에 대해 데이터 증강 수행\"\"\"\n",
    "    augmented_windows = []\n",
    "    data = window[['x', 'y', 'z']].values\n",
    "    for _ in range(augment_factor):\n",
    "        for method in methods:\n",
    "            if method == 'noise':\n",
    "                noise = np.random.normal(0, 0.01, data.shape)\n",
    "                aug_data = data + noise\n",
    "            elif method == 'shift':\n",
    "                shift = np.random.randint(1, int(data.shape[0] * 0.2) + 1)\n",
    "                aug_data = np.roll(data, shift, axis=0)\n",
    "            elif method == 'scaling':\n",
    "                scale = np.random.uniform(0.9, 1.1)\n",
    "                aug_data = data * scale\n",
    "            else:\n",
    "                aug_data = data.copy()\n",
    "            \n",
    "            aug_window = pd.DataFrame(aug_data, columns=['x', 'y', 'z'], index=window.index)\n",
    "            augmented_windows.append(aug_window)\n",
    "    return augmented_windows\n",
    "\n",
    "def get_time_segments_by_blocks(data: pd.DataFrame, config: Config) -> Dict[str, List[Tuple[pd.Timestamp, pd.Timestamp]]]:\n",
    "    \"\"\"데이터를 20초 블록으로 나누고, 각 블록의 train/val/test 시간 구간 반환\"\"\"\n",
    "    segments = {'train': [], 'val': [], 'test': []}\n",
    "    total_seconds = (data.index[-1] - data.index[0]).total_seconds()\n",
    "\n",
    "    for i in range(int(total_seconds / config.block_size_seconds) + 1):\n",
    "        block_start = data.index[0] + timedelta(seconds=i * config.block_size_seconds)\n",
    "        block_end = block_start + timedelta(seconds=config.block_size_seconds)\n",
    "        block_data = data[(data.index >= block_start) & (data.index < block_end)]\n",
    "        \n",
    "        if len(block_data) > 0:\n",
    "            n = len(block_data)\n",
    "            train_end_idx = int(n * config.train_ratio)\n",
    "            val_end_idx = train_end_idx + int(n * config.val_ratio)\n",
    "            \n",
    "            if train_end_idx > 0:\n",
    "                segments['train'].append((block_data.index[0], block_data.index[train_end_idx - 1]))\n",
    "            if val_end_idx > train_end_idx:\n",
    "                segments['val'].append((block_data.index[train_end_idx], block_data.index[val_end_idx - 1]))\n",
    "            if n > val_end_idx:\n",
    "                segments['test'].append((block_data.index[val_end_idx], block_data.index[-1]))\n",
    "                \n",
    "    return segments\n",
    "\n",
    "#####################################\n",
    "# 3. 데이터 변환 및 저장 함수\n",
    "#####################################\n",
    "\n",
    "def resample_window(window: pd.DataFrame, config: Config) -> pd.DataFrame:\n",
    "    \"\"\"윈도우를 target_fs 기준으로 리샘플링\"\"\"\n",
    "    num_samples = len(window)\n",
    "    end_time = num_samples / config.fs\n",
    "    new_time = np.arange(0, end_time, 1 / config.fs)\n",
    "    if len(new_time) < 2: return pd.DataFrame() # 리샘플링 불가 케이스\n",
    "    \n",
    "    resampled_signal = resample(window[['x', 'y', 'z']].values, len(new_time), axis=0)\n",
    "    resampled_window = pd.DataFrame(resampled_signal, columns=['x', 'y', 'z'])\n",
    "    resampled_window['time'] = pd.to_datetime(new_time, unit='s')\n",
    "    return resampled_window.set_index('time')\n",
    "\n",
    "def window_to_stft_image(window: pd.DataFrame, config: Config) -> np.ndarray:\n",
    "    \"\"\"윈도우에서 STFT 이미지(3채널) 생성\"\"\"\n",
    "    noverlap = config.nperseg - config.hop_length\n",
    "    stft_images = []\n",
    "    for axis in ['x', 'y', 'z']:\n",
    "        f, t, Zxx = stft(window[axis].values, fs=config.fs, nperseg=config.nperseg, noverlap=noverlap)\n",
    "        freq_idx = f <= config.freq_limit\n",
    "        stft_img = np.abs(Zxx[freq_idx, :])\n",
    "        \n",
    "        # 시간 축 길이 맞추기 (패딩 또는 절삭)\n",
    "        if stft_img.shape[1] < config.target_time_bins:\n",
    "            padding = np.zeros((stft_img.shape[0], config.target_time_bins - stft_img.shape[1]))\n",
    "            stft_img = np.concatenate((stft_img, padding), axis=1)\n",
    "        else:\n",
    "            stft_img = stft_img[:, :config.target_time_bins]\n",
    "        stft_images.append(stft_img)\n",
    "    return np.stack(stft_images, axis=0)\n",
    "\n",
    "def window_to_raw_signal(window: pd.DataFrame, config: Config) -> np.ndarray:\n",
    "    \"\"\"윈도우에서 Raw 신호(3채널) 생성\"\"\"\n",
    "    values = window[['x', 'y', 'z']].values.T  # shape: (3, L)\n",
    "    target_raw_length = int(config.window_duration_seconds * config.fs)\n",
    "    if values.shape[1] != target_raw_length:\n",
    "        values = resample(values, target_raw_length, axis=1)\n",
    "    return values\n",
    "\n",
    "def save_data_for_split(class_name: str, split_name: str, windows: List[pd.DataFrame], metadata: List[Dict], config: Config):\n",
    "    \"\"\"지정된 split의 윈도우들을 STFT와 Raw 신호로 변환하여 저장\"\"\"\n",
    "    if not windows:\n",
    "        print(f\"  ⏩ No windows to save for class '{class_name}', split '{split_name}'.\")\n",
    "        return\n",
    "\n",
    "    stft_tensors, raw_tensors = [], []\n",
    "    for window in windows:\n",
    "        resampled = resample_window(window, config)\n",
    "        if not resampled.empty:\n",
    "            stft_tensors.append(window_to_stft_image(resampled, config))\n",
    "            raw_tensors.append(window_to_raw_signal(resampled, config))\n",
    "\n",
    "    # STFT 저장\n",
    "    stft_dir = os.path.join(config.output_stft_dir, split_name)\n",
    "    os.makedirs(stft_dir, exist_ok=True)\n",
    "    stft_path = os.path.join(stft_dir, f\"{class_name}_stft_tensors.npy\")\n",
    "    np.save(stft_path, np.stack(stft_tensors))\n",
    "\n",
    "    # Raw 신호 저장\n",
    "    raw_dir = os.path.join(config.output_raw_dir, split_name)\n",
    "    os.makedirs(raw_dir, exist_ok=True)\n",
    "    raw_path = os.path.join(raw_dir, f\"{class_name}_raw_tensors.npy\")\n",
    "    np.save(raw_path, np.stack(raw_tensors))\n",
    "\n",
    "    print(f\"  💾 Saved {class_name}/{split_name}: {len(windows)} windows ({len(stft_tensors)} STFT, {len(raw_tensors)} Raw)\")\n",
    "    \n",
    "    # 메타데이터 저장\n",
    "    meta_df = pd.DataFrame(metadata)\n",
    "    meta_filename = f\"{class_name}_window_metadata_{split_name}.csv\"\n",
    "    meta_df_path = os.path.join(config.output_raw_dir, meta_filename)\n",
    "    meta_df.to_csv(meta_df_path, index=False)\n",
    "    print(f\"  📝 Saved metadata: {meta_df_path}\")\n",
    "\n",
    "#####################################\n",
    "# 4. 데이터 처리 워크플로우 함수\n",
    "#####################################\n",
    "\n",
    "def generate_windows_for_class(class_name: str, files: Dict[str, str], config: Config, \n",
    "                               augment_splits: Dict[str, int] = {}) -> Tuple[Dict[str, list], List[Dict]]:\n",
    "    \"\"\"\n",
    "    클래스에 속한 모든 파일에 대해 윈도우를 생성하고, 지정된 split에 증강 적용\n",
    "    augment_splits: {'train': 5, 'val': 1} 처럼 증강할 split과 계수를 지정\n",
    "    \"\"\"\n",
    "    class_windows = {'train': [], 'val': [], 'test': []}\n",
    "    class_metadata = []\n",
    "\n",
    "    for file_id, file_path in files.items():\n",
    "        print(f\"    📄 Processing file: {file_id}\")\n",
    "        data = load_data(file_path)\n",
    "        time_segments = get_time_segments_by_blocks(data, config)\n",
    "\n",
    "        for split, segments in time_segments.items():\n",
    "            for seg_idx, (start_time, end_time) in enumerate(segments):\n",
    "                block_data = data.loc[start_time:end_time]\n",
    "                if block_data.empty: continue\n",
    "\n",
    "                atoms = group_atoms(block_data)\n",
    "                windows = create_sliding_windows(atoms, config.window_size_atoms, config.step)\n",
    "                \n",
    "                # 원본 윈도우 추가\n",
    "                class_windows[split].extend(windows)\n",
    "                \n",
    "                # 메타데이터 기록\n",
    "                for win_idx, window in enumerate(windows):\n",
    "                    class_metadata.append({\n",
    "                        \"class\": class_name, \"file\": file_id, \"split\": split, \"block_index\": seg_idx,\n",
    "                        \"window_idx\": win_idx, \"start_time\": window.index[0], \"end_time\": window.index[-1],\n",
    "                        \"is_augmented\": False\n",
    "                    })\n",
    "                \n",
    "                # 증강 적용\n",
    "                augment_factor = augment_splits.get(split, 0)\n",
    "                if augment_factor > 0:\n",
    "                    augmented_wins = []\n",
    "                    for window in windows:\n",
    "                        augmented_wins.extend(augment_window(window, augment_factor))\n",
    "                    class_windows[split].extend(augmented_wins)\n",
    "                    print(f\"      ➕ Augmented {split} with {len(augmented_wins)} windows (factor={augment_factor})\")\n",
    "\n",
    "    return class_windows, class_metadata\n",
    "\n",
    "def run_processing(config: Config):\n",
    "    \"\"\"Train/Val/Test 데이터셋을 생성. Train set의 abnormal_1,2는 증강\"\"\"\n",
    "    print(\"\\n🚀 Starting Data Processing (Train/Val/Test)\")\n",
    "    for class_name, files in config.all_files.items():\n",
    "        print(f\"  🔹 Processing class: {class_name}\")\n",
    "        \n",
    "        # abnormal_1, 2의 train split에만 증강 적용\n",
    "        augment_config = {}\n",
    "        if class_name in config.classes_to_augment:\n",
    "            augment_config = {'train': 5}\n",
    "            print(f\"    ✨ Augmentation will be applied to 'train' split.\")\n",
    "\n",
    "        all_windows, all_metadata = generate_windows_for_class(class_name, files, config, augment_splits=augment_config)\n",
    "        \n",
    "        for split in ['train', 'val', 'test']:\n",
    "            split_windows = all_windows[split]\n",
    "            split_metadata = [m for m in all_metadata if m['split'] == split]\n",
    "            save_data_for_split(class_name, split, split_windows, split_metadata, config)\n",
    "\n",
    "#####################################\n",
    "# 5. 결과 확인 및 메인 실행 함수\n",
    "#####################################\n",
    "\n",
    "def print_sample_counts(config: Config):\n",
    "    \"\"\"각 split 디렉토리별 샘플 수 출력\"\"\"\n",
    "    splits_to_check = ['train', 'val', 'test']\n",
    "    class_names = list(config.all_files.keys())\n",
    "    \n",
    "    print(\"\\n📊 Sample Count Summary\")\n",
    "    print(\"─────────────────────────────────────────────────────────────\")\n",
    "    print(f\"{'Split':<10} | {'Class':<12} | {'STFT':>6} | {'Raw':>6}\")\n",
    "    print(\"─────────────────────────────────────────────────────────────\")\n",
    "\n",
    "    for split in splits_to_check:\n",
    "        for cls in class_names:\n",
    "            stft_path = os.path.join(config.output_stft_dir, split, f\"{cls}_stft_tensors.npy\")\n",
    "            raw_path = os.path.join(config.output_raw_dir, split, f\"{cls}_raw_tensors.npy\")\n",
    "            \n",
    "            stft_count = len(np.load(stft_path)) if os.path.exists(stft_path) else 0\n",
    "            raw_count = len(np.load(raw_path)) if os.path.exists(raw_path) else 0\n",
    "            \n",
    "            if stft_count > 0 or raw_count > 0:\n",
    "                print(f\"{split:<10} | {cls:<12} | {stft_count:6d} | {raw_count:6d}\")\n",
    "    print(\"─────────────────────────────────────────────────────────────\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d91d3376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Starting Data Processing (Train/Val/Test)\n",
      "  🔹 Processing class: normal\n",
      "    📄 Processing file: normal_1\n",
      "    📄 Processing file: normal_2\n",
      "    📄 Processing file: normal_3\n",
      "    📄 Processing file: normal_4\n",
      "    📄 Processing file: normal_5\n",
      "    📄 Processing file: normal_6\n",
      "    📄 Processing file: normal_7\n",
      "    📄 Processing file: normal_8\n",
      "    📄 Processing file: normal_9\n",
      "    📄 Processing file: normal_10\n",
      "    📄 Processing file: normal_11\n",
      "    📄 Processing file: normal_12\n",
      "    📄 Processing file: normal_13\n",
      "    📄 Processing file: normal_14\n",
      "    📄 Processing file: normal_15\n",
      "    📄 Processing file: normal_16\n",
      "    📄 Processing file: normal_17\n",
      "    📄 Processing file: normal_18\n",
      "    📄 Processing file: normal_19\n",
      "    📄 Processing file: normal_20\n",
      "  💾 Saved normal/train: 1904 windows (1904 STFT, 1904 Raw)\n",
      "  📝 Saved metadata: processed_data_raw/normal_window_metadata_train.csv\n",
      "  💾 Saved normal/val: 156 windows (156 STFT, 156 Raw)\n",
      "  📝 Saved metadata: processed_data_raw/normal_window_metadata_val.csv\n",
      "  💾 Saved normal/test: 156 windows (156 STFT, 156 Raw)\n",
      "  📝 Saved metadata: processed_data_raw/normal_window_metadata_test.csv\n",
      "  🔹 Processing class: abnormal_1\n",
      "    ✨ Augmentation will be applied to 'train' split.\n",
      "    📄 Processing file: abnormal_1_1\n",
      "      ➕ Augmented train with 180 windows (factor=5)\n",
      "      ➕ Augmented train with 180 windows (factor=5)\n",
      "    📄 Processing file: abnormal_1_2\n",
      "      ➕ Augmented train with 180 windows (factor=5)\n",
      "      ➕ Augmented train with 180 windows (factor=5)\n",
      "      ➕ Augmented train with 180 windows (factor=5)\n",
      "      ➕ Augmented train with 45 windows (factor=5)\n",
      "    📄 Processing file: abnormal_1_3\n",
      "      ➕ Augmented train with 180 windows (factor=5)\n",
      "      ➕ Augmented train with 180 windows (factor=5)\n",
      "      ➕ Augmented train with 180 windows (factor=5)\n",
      "      ➕ Augmented train with 0 windows (factor=5)\n",
      "  💾 Saved abnormal_1/train: 1584 windows (1584 STFT, 1584 Raw)\n",
      "  📝 Saved metadata: processed_data_raw/abnormal_1_window_metadata_train.csv\n",
      "  💾 Saved abnormal_1/val: 8 windows (8 STFT, 8 Raw)\n",
      "  📝 Saved metadata: processed_data_raw/abnormal_1_window_metadata_val.csv\n",
      "  💾 Saved abnormal_1/test: 8 windows (8 STFT, 8 Raw)\n",
      "  📝 Saved metadata: processed_data_raw/abnormal_1_window_metadata_test.csv\n",
      "  🔹 Processing class: abnormal_2\n",
      "    ✨ Augmentation will be applied to 'train' split.\n",
      "    📄 Processing file: abnormal_2_1\n",
      "      ➕ Augmented train with 180 windows (factor=5)\n",
      "      ➕ Augmented train with 180 windows (factor=5)\n",
      "      ➕ Augmented train with 180 windows (factor=5)\n",
      "      ➕ Augmented train with 180 windows (factor=5)\n",
      "      ➕ Augmented train with 180 windows (factor=5)\n",
      "      ➕ Augmented train with 180 windows (factor=5)\n",
      "      ➕ Augmented train with 165 windows (factor=5)\n",
      "      ➕ Augmented train with 180 windows (factor=5)\n",
      "      ➕ Augmented train with 180 windows (factor=5)\n",
      "      ➕ Augmented train with 105 windows (factor=5)\n",
      "    📄 Processing file: abnormal_2_2\n",
      "      ➕ Augmented train with 180 windows (factor=5)\n",
      "      ➕ Augmented train with 180 windows (factor=5)\n",
      "      ➕ Augmented train with 180 windows (factor=5)\n",
      "      ➕ Augmented train with 180 windows (factor=5)\n",
      "      ➕ Augmented train with 180 windows (factor=5)\n",
      "      ➕ Augmented train with 45 windows (factor=5)\n",
      "  💾 Saved abnormal_2/train: 2832 windows (2832 STFT, 2832 Raw)\n",
      "  📝 Saved metadata: processed_data_raw/abnormal_2_window_metadata_train.csv\n",
      "  💾 Saved abnormal_2/val: 14 windows (14 STFT, 14 Raw)\n",
      "  📝 Saved metadata: processed_data_raw/abnormal_2_window_metadata_val.csv\n",
      "  💾 Saved abnormal_2/test: 14 windows (14 STFT, 14 Raw)\n",
      "  📝 Saved metadata: processed_data_raw/abnormal_2_window_metadata_test.csv\n",
      "  🔹 Processing class: abnormal_3\n",
      "    📄 Processing file: abnormal_3_1\n",
      "    📄 Processing file: abnormal_3_2\n",
      "  💾 Saved abnormal_3/train: 1955 windows (1955 STFT, 1955 Raw)\n",
      "  📝 Saved metadata: processed_data_raw/abnormal_3_window_metadata_train.csv\n",
      "  💾 Saved abnormal_3/val: 165 windows (165 STFT, 165 Raw)\n",
      "  📝 Saved metadata: processed_data_raw/abnormal_3_window_metadata_val.csv\n",
      "  💾 Saved abnormal_3/test: 165 windows (165 STFT, 165 Raw)\n",
      "  📝 Saved metadata: processed_data_raw/abnormal_3_window_metadata_test.csv\n",
      "  🔹 Processing class: abnormal_4\n",
      "    📄 Processing file: abnormal_4_3\n",
      "    📄 Processing file: abnormal_4_4\n",
      "    📄 Processing file: abnormal_4_5\n",
      "  💾 Saved abnormal_4/train: 2249 windows (2249 STFT, 2249 Raw)\n",
      "  📝 Saved metadata: processed_data_raw/abnormal_4_window_metadata_train.csv\n",
      "  💾 Saved abnormal_4/val: 190 windows (190 STFT, 190 Raw)\n",
      "  📝 Saved metadata: processed_data_raw/abnormal_4_window_metadata_val.csv\n",
      "  💾 Saved abnormal_4/test: 189 windows (189 STFT, 189 Raw)\n",
      "  📝 Saved metadata: processed_data_raw/abnormal_4_window_metadata_test.csv\n",
      "\n",
      "📊 Sample Count Summary\n",
      "─────────────────────────────────────────────────────────────\n",
      "Split      | Class        |   STFT |    Raw\n",
      "─────────────────────────────────────────────────────────────\n",
      "train      | normal       |   1904 |   1904\n",
      "train      | abnormal_1   |   1584 |   1584\n",
      "train      | abnormal_2   |   2832 |   2832\n",
      "train      | abnormal_3   |   1955 |   1955\n",
      "train      | abnormal_4   |   2249 |   2249\n",
      "val        | normal       |    156 |    156\n",
      "val        | abnormal_1   |      8 |      8\n",
      "val        | abnormal_2   |     14 |     14\n",
      "val        | abnormal_3   |    165 |    165\n",
      "val        | abnormal_4   |    190 |    190\n",
      "test       | normal       |    156 |    156\n",
      "test       | abnormal_1   |      8 |      8\n",
      "test       | abnormal_2   |     14 |     14\n",
      "test       | abnormal_3   |    165 |    165\n",
      "test       | abnormal_4   |    189 |    189\n",
      "─────────────────────────────────────────────────────────────\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. 설정 객체 생성\n",
    "    config = Config()\n",
    "\n",
    "    # 2. Train/Val/Test 데이터셋 생성\n",
    "    # (abnormal_1, abnormal_2의 train set은 내부적으로 증강됨)\n",
    "    run_processing(config)\n",
    "\n",
    "    # 3. 최종 결과 확인\n",
    "    print_sample_counts(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11eb8540",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
